# 提升知识点

手写nms

链接：https://www.nowcoder.com/discuss/72025?type=2&order=0&pos=10&page=1
问了CNN的项目，NIN的特点，还有NIN比VGG好在哪里，我一直说不到他想要的答案（醉了）
问了如何减少overfit 我说了dropout 和 dropconnect， 面试官好像没听过dropconnect，以为我说的这两个是一个东西，并且理解有误（醉了）
问了HOG算法原理，HOG变种（不会），HOG简化计算方式（不会，但是会Haar的积分图），讲了积分图，期间提到算法我自己实现过，面试官质疑自己实现应该很清楚才对啊（醉了）
最后一道算法题，0-25表示A-Z，给一个数字字符串，求有多少种解码方式（dp问题，思路对了，关键步骤没写对，直接挂了）


链接：https://www.nowcoder.com/discuss/83377
一直在问faster rcnn，faster rcnn RPN流程，anchor选的太大或太小有什么影响，正负样本选取的时候为什么用0.7和0.3的超参，与nms使用0.3的超参是否有关，smooth l1损失为什么不用l1，为什么不用l2，w，h回归的时候为什么要用log，x，y回归的时候为什么用除法等等。。


x,y属于[0,1]的均匀分布，求max（x,y）的期望


链接：https://www.nowcoder.com/discuss/58508
头条 ai lab
1、leetcode原题 多叉树只有连接关系pair<*treenode, *treenode> 如何找到最长的路径， 会议室时间分配问题，需要几个会议室 
2、很多深度学习的知识，很深，比如 bn，写出公式、为什么需要额外的两个参数b和y、测试的时候怎么做的、参数怎么存储的、均值方差用的是训练哪一阶段的、如何更新参数、caffe中BN的实现分几层、测试的时候是否可以将卷积核BN参数合并？公式是什么，caffe中如何修改参数、如果batch只能容纳一个图像怎么训练。恩，我只是举了一个问题的例子，有很多类似问题 代码，计算两个候选框的overlap
3、手写k means代码，哪些操作可以防止过拟合，知道哪些网络结构等很基础的知识



链接：https://www.nowcoder.com/discuss/80846
一面： 照常先聊了项目然后问了一道算法题，先出了简单情况： 有三种砖块，长度分别为1，2，3，砖块高度都为1，现在要你砌一面M*N的墙，不考虑把砖块竖着放的情况，问有多少种砌法。（DP可解） 然后接着问：如果要求这面墙除了两边外，中间任何一个位置砖的边缘都不能构成一条从墙顶到地面的直线，那么有多少种砌法？（DP+容斥原理） 我顺利做出来了，一面over，几分钟后二面。
二面： 聊了会项目，然后问了我比较新的深度网络你知道哪些，然后问了我好多faster-RCNN的细节，之后问我fasterRCNN做了region pooling后，图片大小不能整除导致中间的特征不好感知到怎么办，我一开始说加padding，面试官不满意，我就说我不知道了，面试官告诉我可以用插值补全图片。 然后考了一道关于CNN感受野的计算，很简单，不过我当时状态不好，没做对，问了面试官，面试官给了个答案，但我觉得也有点问题，就和他争了一会，然后就结束了，当时就感觉药丸了。 几天后hr通知我5.2号三面，简直是柳暗花明又一村啊！
三面： 三面面试官迟到了，是个大叔。应该是某个组的leader，就让我介绍了一下项目，然后问了我CNN和RNN，让我介绍了一下LSTM，然后大叔就说我这边结束了，以后再联系。我很惊讶还问了一句：“不考算法题吗？”大叔说不考不考。估计大叔还有事吧。
hr 面： 头条效率很高，三面结束一天后，5.3就进行了hr面，聊了十分钟人生规划和性格就结束了，hr小姐姐和我说一周出结果。 5.10号我问了hr姐姐，hr姐姐说我通过了，offer报批中，yeah！然后一天后发了邮件正式offer。



-------------------------------------------------------------------



排序稳定性，复杂度，   归并，堆排序，快排
python所有的关键字 \__getitem__   \__call__含义

python Method Resolution
手推神经网络

递归的使用，在排列组合里面
宽度优先搜索高级模板
动归

python resnet zeropadding是否需要新参数？
卷积时间复杂度
宽度优先搜索的时间复杂度



Resnet的bottleeck和buildingblock对比
1. bottleneck 维度更深
2. 时间计算复杂度一样
3. bottleneck维度更深，输入输出的参数更多。空间复杂度高
也就是说bottle计算复杂度不高，可以学习更深特在，但是空间复杂度高（输入输出参数多）

以两个(3x3,64)的卷积 输入输出都是64  和 三个（1x1,64) (3x3,64), (1x1, 256)  输入输出都是256 为例子


**空间复杂度

卷积层参数只和 kernelsize，出入channel有关， 参数量 = (kernel_size * kernel_size * inchannel + 1) * outchannel
buildblock  (3*3*64+1）*64  *2 = 73856
bottleneck  (1*1*256+1)*64 + (3*3*64+1)*64 + (1x1x64+1)*256 =  70016

**时间复杂度

因为两种结构的输出尺寸都不发生变化，所以时间复杂度也一样


bottleneck的设计是这样的，虽然两边输入都是256,但是在3x3卷积的周围其实输入出都是64的。 256维度的降低和increase都是用1x1卷积完成的
这样可以减少参数。否则两个256*256就需要非常多的参数了。 **首端和末端的1x1卷积用来削减和恢复维度**


时间复杂度
注1：为了简化表达式中的变量个数，这里统一假设输入和卷积核的形状都是正方形。 
注2：严格来讲每层应该还包含1个Bias参数，这里为了简洁就省略了。
M:输出特征图（Feature Map）的尺寸。
K:卷积核（Kernel）的尺寸。
Cin:输入通道数。
Cout:输出通道数。
横向做 M2次运算，  纵向做Cout次运算  每次cube运算是 K2*Cin
M^2*K^2*Cin*Cout = O(M^2 * K^2 * Cin * Cout)

空间复杂度
空复杂度复杂度：Time~O(K^2 * Cin * Cout)   因为参数共享了，所有的卷积层K都只有一个，所以M^2次运算并不影响空间复杂度。


# Past Puzzles

--------------------------0811----------------------
```
8邻域编码
卡方检验
abcde依次入栈，b开头的序列有多少
细化，开运算，闭运算区别
排序稳定性
堆排序复杂度
重采样？上采样？
有向图的邻结矩阵可以计算某个节点的 出度入度么？
t分布，高斯分布
后续中序遍历，推先序序列
二差搜索数和二分的关联。
中小数据集用什么模型好？

一道排列组合题，递归思想

深度学习综述方法方向理解
计算机视觉流程。
```

--------------------------0812   NE Lab----------------------
```
欠拟合  过拟合  和 (variance, bias)关系
boosting bagging有没有类似nn dropout效果？
cnn共享卷积有没有抗拟合效果？
Representation learning指的是那些？ Rf gbdt nn？
dqn的经验回放理解
一个python list 为a，len(a)操作和 a[-1] = 0的时间复杂度  （https://wiki.python.org/moin/TimeComplexity）
gbdt和xgboost区别
random walk 从1到10, 每次可以50%一步，或者移动两步，请问到4停止需要多少步（期望）
softmax 输入是否可以输入全都是0
hmm 知道观察序列和状态序列，用什么方法来估计参数
resnet zero padding 会不会导致参数增多
宽搜的时间复杂度
word2vec, fasttext,等那种方法适合处理oov（out of vacabulary)
VAE
t-sne和pca各自优缺点 适用数据情况

大题
一个（4,4,6,16）卷积的权重参数个数计算。
dqn设计flappybird思路
宽度优先搜索大题
Lstm  推导


```


### Thoughts 思考

#### 内排序 外派序-排序稳定性
* 所谓的内排序是指所有的数据已经读入内存，在内存中进行排序的算法。排序过程中不需要对磁盘进行读写。同时，内排序也一般假定所有用到的辅助空间也可以直接存在于内存中。
* 与之对应地，另一类排序称作外排序，即内存中无法保存全部数据，需要进行磁盘访问，每次读入部分数据到内存进行排序。

一般归并排序空间复杂度为o(n) 占用空间较多，不经常作为内排序，而是作为外派序用到
而快排空间为o(1) ，经常做为内排序。

插入排序是o(n2)的时间复杂度
o(nlogn) 堆排序（o(n)空间。不稳定）     归并排序(o(n+logn))空间,稳定排序）    和冒泡排序(o(1) 不稳定)


### resnet 参数free---------------------------
resnet的zero_padding是参数free的，不增加新的学习参数
